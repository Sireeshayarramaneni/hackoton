# -*- coding: utf-8 -*-
"""hackoton.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19SI2ocMLtKcs_dsF6VLZaBT0BxRCbUvK
"""

#---------MEDIUM QUESTION---------------

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

df=pd.read_csv ("/content/housing_prices.csv")
df

df.info()

df.isnull().sum()

df.describe()

#finding duplicate values
df[df.duplicated()]

#removing unwanted data
df.drop(["prefarea"],axis=1,inplace=True)
df

df.info()

sns.boxplot(df['price'])

sns.boxplot(x=df["area"])
plt.show()

sns.boxplot(x=df["bedrooms"])
plt.show()

sns.boxplot(x=df["stories"])
plt.show()

sns.boxplot(x=df["price"])
plt.show()

sns.boxplot(df['parking'])
plt.show()

col=['price','area']
from scipy.stats import zscore
for i in col:
  df[i]=np.abs(zscore(df[i]))
  df.info()

df.describe()

from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
col = ['mainroad','guestroom','basement','furnishingstatus','hotwaterheating','airconditioning']
for k in col:
  df[k] = encoder.fit_transform(df[k])

df1 = df.select_dtypes(np.number)
df1.corr()

#bargraph
plt.figure(figsize=(20,8))
sns.barplot(x="furnishingstatus",y="price",data=df,color="blue",width=0.2)
plt.show()

plt.figure(figsize=(20,8))
sns.barplot(x="parking",y="price",data=df,color="pink",width=0.2)
plt.show()

plt.figure(figsize=(20,8))
plt.scatter(x=df["area"],y=df["price"],color="purple")
plt.xlabel("Area")
plt.ylabel("Price of House")
plt.show()

sns.pairplot(df)
plt.show()

sns.heatmap(df.corr(),annot=True, cmap='coolwarm' ,fmt=".2f")
plt.title('Correlation Heatmap')
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
col1 = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking']
x = df[col1]
y = df['price']
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.4)
model = LinearRegression()
model.fit(x_train, y_train)
print(model.score(x_train,y_train))
y_pred = model.predict(x_test)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print("Mean Squared Error:", mse)
print("R^2 Score:", r2)

from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor()
model.fit(x_train,y_train)
print(model.score(x_train,y_train))
y_pred=model.predict(x_test)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print("Mean Squared Error:", mse)
print("R^2 Score:", r2)

from sklearn.tree import DecisionTreeRegressor

model = DecisionTreeRegressor()
model.fit(x_train,y_train)
print(model.score(x_train,y_train))
y_pred=model.predict(x_test)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print("Mean Squared Error:", mse)
print("R^2 Score:", r2)

#--------------HARD QUESTION----------------

import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer

# Download NLTK resources
nltk.download('stopwords')
nltk.download('punkt')

df=pd.read_csv('/content/sentiment_dataset (2).csv')
df

df.shape

# Data cleaning and preprocessing
def preprocess_text(text):
 # Remove URLs
    text = re.sub(r'http\S+', '', text)
 # Remove special characters
    text = re.sub(r'\W', ' ', text)
# Remove extra whitespaces
    text = re.sub(r'\s+', ' ', text)
# Convert to lowercase
    text = text.lower().strip()
    return text

df.info()

def clean_text(text):
    # Remove URLs
    text=str(text)
    text = re.sub(r'http\S+', '', text)
    # Remove mentions and hashtags
    text = re.sub(r'@[A-Za-z0-9_]+|#[A-Za-z0-9_]+', '', text)
    # Remove special characters and numbers
    text = re.sub(r'[^a-zA-Z]', ' ', text)
    # Tokenize the text
    tokens = word_tokenize(text)
    # Convert to lowercase
    tokens = [word.lower() for word in tokens]
    # Remove stop words
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word not in stop_words]
# Stemming
    stemmer = PorterStemmer()
    tokens = [stemmer.stem(word) for word in tokens]
    # Join tokens back into the data
    cleaned_text = ' '.join(tokens)
    return cleaned_text

# Apply cleaning function to 'tweet' column
df['cleaned_tweet'] = df['tweet'].apply(clean_text)

df.describe()

from sklearn.feature_extraction.text import TfidfVectorizer

# Initialize TF-IDF vectorizer
tfidf_vectorizer = TfidfVectorizer(max_features=5000)

# Fit and transform the cleaned text data
tfidf_features = tfidf_vectorizer.fit_transform(df['cleaned_tweet']).toarray()

from textblob import TextBlob

# Function to perform sentiment analysis using TextBlob
def analyze_sentiment(text):
    blob = TextBlob(text)
    polarity = blob.sentiment.polarity
    if polarity > 0:
        return 'positive'
    elif polarity < 0:
        return 'negative'
    else:
        return 'neutral'

# Apply sentiment analysis to each step in the dataset
df['sentiment'] = df['cleaned_tweet'].apply(analyze_sentiment)

# Display the dataframe
df.head()

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.linear_model import LogisticRegression
print(df.columns)

#Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(tfidf_features, df['sentiment'], test_size=0.2, random_state=42)

# Initialize Logistic Regression model
lr_model = LogisticRegression()

# Train the model
lr_model.fit(X_train, y_train)

# Predict on the test set
y_pred = lr_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
print("Classification Report:")
print(classification_report(y_test, y_pred))

